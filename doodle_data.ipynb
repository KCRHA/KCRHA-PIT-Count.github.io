{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a09236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import Workbook\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370506ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System detected: <module 'os' (frozen)>\n"
     ]
    }
   ],
   "source": [
    "# set the relative path for the exports\n",
    "# -------------------------------------\n",
    "# base path choice on OS\n",
    "print(f\"Operating System detected: {os}\")\n",
    "\n",
    "if os.name == 'nt':  # Windows\n",
    "    rel_path = \"C://Users/ben_g/OneDrive - King County Regional Homelessness Authority/Research and Data - 2026 Unsheltered PIT/Volunteer Training & Schedule/Doodle Exports\"\n",
    "    # set path for output\n",
    "    filepath = 'C://Users/ben_g/OneDrive - King County Regional Homelessness Authority/Research and Data - 2026 Unsheltered PIT/Volunteer Training & Schedule/Volunteer Schedules - Doodle.xlsx'\n",
    "else:  # macOS or Linux\n",
    "    rel_path = \"/Users/ben.mathewson/Library/CloudStorage/OneDrive-KingCountyRegionalHomelessnessAuthority/Research and Data - 2026 Unsheltered PIT/Volunteer Training & Schedule/Doodle Exports\"\n",
    "    # set path for output\n",
    "    filepath = '/Users/ben.mathewson/Library/CloudStorage/OneDrive-KingCountyRegionalHomelessnessAuthority/Research and Data - 2026 Unsheltered PIT/Volunteer Training & Schedule/Volunteer Schedules - Doodle.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fca55b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ben.mathewson/Library/CloudStorage/OneDrive-KingCountyRegionalHomelessnessAuthority/Research and Data - 2026 Unsheltered PIT/Volunteer Training & Schedule/Volunteer Schedules - Doodle.xlsx'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/ben.mathewson/Library/CloudStorage/OneDrive-KingCountyRegionalHomelessnessAuthority/Research and Data - 2026 Unsheltered PIT/Volunteer Training & Schedule/Doodle Exports'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(filepath)\n",
    "display(rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76beec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# loop over the list of workbooks\n",
    "subregions = [\n",
    "    \"Vashon Island\",\n",
    "    \"South Seattle\",\n",
    "    \"South King County\",\n",
    "    \"South East King County\",\n",
    "    \"Snoqualmie Valley\",\n",
    "    \"North Seattle\",\n",
    "    \"North King County\",\n",
    "    \"Family Phone Line\",\n",
    "    \"East King County\",\n",
    "    \"Downtown Seattle\"\n",
    "]\n",
    "\n",
    "# ensure the working file exists\n",
    "if not os.path.exists(filepath):\n",
    "    # If neither exists, create a fresh empty workbook\n",
    "    wb = Workbook()\n",
    "    wb.save(filepath)\n",
    "    print(\"Created new working file.\")\n",
    "\n",
    "# create dataframe to concatentate workbooks into\n",
    "all_regions = pd.DataFrame()\n",
    "\n",
    "# loop over the list, creating an Excel workbook of results\n",
    "# --------------\n",
    "for region in subregions:\n",
    "    # get the source export\n",
    "    path = f\"{rel_path}/{region}.xlsx\" # MAY NEED TO SPECIFY\n",
    "    # read into a pandas dataframe\n",
    "    workbook = pd.read_excel(io=path,\n",
    "                            header=5,\n",
    "                            index_col=0,\n",
    "                            engine='openpyxl')\n",
    "    \n",
    "    # do a little cleaning\n",
    "    workbook_stripped = workbook.drop(columns=[\"email\",\n",
    "                                    \"Volunteer Emergency Contact Name (Last, First)\",\n",
    "                                    \"Volunteer Emergency Contact Phone Number\",\n",
    "                                    \"Volunteer Phone Number\",\n",
    "                                    \"Waiver Completed?\"])\n",
    "    \n",
    "    # transpose the dataframe\n",
    "    workbook_stripped = workbook_stripped.T\n",
    "\n",
    "    # reset the index\n",
    "    workbook_stripped = workbook_stripped.reset_index(names=\"DateTime\")\n",
    "\n",
    "    # split 'DateTime' into two columns\n",
    "    # ----------------------------------\n",
    "    # look for a space preceded by 4 digits\n",
    "    split_data = workbook_stripped['DateTime'].str.split(r'(?<=\\d{4})\\s', expand=True)\n",
    "\n",
    "    workbook_stripped['Date'] = split_data[0]\n",
    "    workbook_stripped['Time'] = split_data[1]\n",
    "\n",
    "    # groupby columns to handle duplicate columns\n",
    "    workbook_stripped = workbook_stripped.groupby(level=0,\n",
    "                                                axis=1).first()\n",
    "    \n",
    "    # replace all \"Signed-up\" with \"Time\"\n",
    "    for col in workbook_stripped.columns:\n",
    "        # print(col)\n",
    "        workbook_stripped.loc[workbook_stripped[col] == \"Signed-up\", col] = workbook_stripped['Time']\n",
    "\n",
    "    # drop newly extraneous columns\n",
    "    workbook_stripped = workbook_stripped.drop(columns=[\"DateTime\",\n",
    "                                                        \"Time\",\n",
    "                                                        \"Total seats\",\n",
    "                                                        \"Participants signed-up\"])\n",
    "\n",
    "    # reset the index to be the \"Date\" col (ahead of transposing)\n",
    "    workbook_stripped = workbook_stripped.set_index(keys=\"Date\")\n",
    "\n",
    "    # add the subregion name as a feature\n",
    "    workbook_stripped[\"Subregion\"] = region\n",
    "\n",
    "    # concatentate to the big dataframe\n",
    "    all_regions = pd.concat([all_regions, workbook_stripped])\n",
    "\n",
    "    # loop over the locations to create dataframes similar to the excel tracker\n",
    "    # ------------------\n",
    "    # get a list of unique locations\n",
    "    locations = list(set(workbook_stripped[\"Session description\"]))\n",
    "\n",
    "    # create a dictionary to hold the indexed dataframes\n",
    "    dfs = {}\n",
    "\n",
    "    # loop over each location to create a separate, named df\n",
    "    for loc in locations:\n",
    "        # create dataframes as transposed copies\n",
    "        # -------------------\n",
    "        # filter for target location\n",
    "        df = workbook_stripped[workbook_stripped[\"Session description\"]==loc].copy()\n",
    "\n",
    "        # drop the \"Session description\" from the newly created df\n",
    "        df = df.drop(columns=[\"Session description\"])\n",
    "\n",
    "        # convert the \"Date\" column to datetime\n",
    "        df.index = pd.to_datetime(df.index,\n",
    "                                    errors=\"coerce\")\n",
    "\n",
    "        # transpose the dataframe and drop empty rows\n",
    "        df = df.T.dropna(how='all')\n",
    "\n",
    "        # group by column and take the 'first' non-null entries\n",
    "        df = df.groupby(level=0,\n",
    "                        axis=1).first()\n",
    "        \n",
    "        # order the dataframe columns chronologically\n",
    "        df = df.sort_index(axis=1)\n",
    "\n",
    "        # reset the index so we don't lose names\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # drop any duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        # remove the row containing 'Subregion'\n",
    "        df = df.drop(index=df[df[\"Name\"]==\"Subregion\"].index, axis=0)\n",
    "\n",
    "        # join back to 'workbook' to preserve email and phone number\n",
    "        workbook = workbook.reset_index() # reset index to restore 'Name' as a column\n",
    "        workbook = workbook[['Name',\n",
    "                            'Volunteer Phone Number',\n",
    "                            'email']] # keep only essential information\n",
    "        df = pd.merge(left=df,\n",
    "                    right=workbook,\n",
    "                    on=\"Name\",\n",
    "                    how=\"left\") # merge the dataframes to capture contact info\n",
    "        \n",
    "        # add placeholder column 'Confirmed'\n",
    "        df[\"Confirmed\"] = \"\" # no content, will appear empty\n",
    "        \n",
    "        # build sorting function for columns\n",
    "        def column_sort_key(col):\n",
    "            if isinstance(col, str):\n",
    "                # (Group 0, Column Name)\n",
    "                return (0, col)\n",
    "            else:\n",
    "                # (Group 1, Timestamp Value)\n",
    "                return (1, col)\n",
    "\n",
    "        # sort name and contact info to the front\n",
    "        new_order = sorted(df.columns, key=column_sort_key)\n",
    "        df = df[new_order]\n",
    "\n",
    "        # change and standardize column labels (format)\n",
    "        df = df.rename(mapper={\"Volunteer Phone Number\":\"Phone\",\n",
    "                            \"email\":\"Email\"},\n",
    "                    axis=1)\n",
    " \n",
    "        # limit the sheet name len to avoid errors\n",
    "        if len(loc)>31:\n",
    "            loc = str(loc[:31])\n",
    "\n",
    "        # Format Timestamp columns to 'Short Date' + (Day Abbreviation)\n",
    "        # Result: 2026-01-26 becomes \"1/26/26 (Mon)\"\n",
    "        new_column_names = []\n",
    "        for col in df.columns:\n",
    "            if isinstance(col, pd.Timestamp):\n",
    "                # %m/%d/%y -> 01/26/26\n",
    "                # %a -> Mon\n",
    "                formatted_date = col.strftime('%m/%d/%y (%a)')\n",
    "                new_column_names.append(formatted_date)\n",
    "            else:\n",
    "                new_column_names.append(col)\n",
    "        \n",
    "        df.columns = new_column_names\n",
    "\n",
    "        # WRITE TO EXCEL WORKBOOK\n",
    "        # -------------------\n",
    "        # create the ExcelWriter object\n",
    "        with pd.ExcelWriter(filepath,\n",
    "                            engine='openpyxl',\n",
    "                            mode='a',\n",
    "                            if_sheet_exists='replace') as writer:\n",
    "            # index=False prevents the 0, 1, 2... row numbers from being saved\n",
    "            df.to_excel(writer, sheet_name=loc, index=False)\n",
    "\n",
    "            # access the openpyxl worksheet object for this specific sheet\n",
    "            worksheet = writer.sheets[loc]\n",
    "            \n",
    "            # iterate through all columns and set the width\n",
    "            # Excel columns are 1-indexed (A=1, B=2, etc.)\n",
    "            for i, col in enumerate(df.columns, 1):\n",
    "                column_letter = worksheet.cell(row=1, column=i).column_letter\n",
    "                \n",
    "                # set width to 25 (Excel units)\n",
    "                worksheet.column_dimensions[column_letter].width = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab91886e",
   "metadata": {},
   "source": [
    "# Transform Compiled Workbooks for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61dc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index so that we can manipulate 'Date'\n",
    "all_regions = all_regions.reset_index(names=\"Date\")\n",
    "\n",
    "# convert 'Date' column to datetime obj\n",
    "all_regions[\"Date\"] = pd.to_datetime(all_regions[\"Date\"], errors='coerce').dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce71ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of volunteer names (for cross-ref to smartsheet)\n",
    "volunteers = list(set(all_regions.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a407468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'Date' and 'Session description' as multiindex before grouping columns\n",
    "all_regions = all_regions.set_index(keys=[\"Date\", \"Session description\", \"Subregion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7715a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby columns to handle duplicate columns\n",
    "grouped_wbk = all_regions.stack().groupby(level=[0,1,2]).value_counts().reset_index(name=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f5561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to handle the time transformation\n",
    "def parse_shift_times(df, column_name='level_3'):\n",
    "    # clean the string: remove the \".1\" duplicates and split by newline\n",
    "    # This separates \"10:00 AM\" from \"4 h\"\n",
    "    temp_split = df[column_name].str.replace(r'\\.\\d+$', '', regex=True).str.split('\\n', expand=True)\n",
    "    \n",
    "    # convert the first part to a datetime (Shift Start)\n",
    "    df['Shift Start'] = pd.to_datetime(temp_split[0], format='%I:%M %p').dt.time\n",
    "    \n",
    "    # convert the second part to a Timedelta (Duration)\n",
    "    # clean the duration string to make it compatible with pandas (e.g., \"4 h 30 min\" -> \"4h 30m\")\n",
    "    duration_str = temp_split[1].str.replace(' h', 'h').str.replace(' min', 'm')\n",
    "    duration_td = pd.to_timedelta(duration_str)\n",
    "    \n",
    "    # calculate Shift End\n",
    "    # combine a dummy date with the start time to do the math, then extract the time\n",
    "    dummy_date = pd.to_datetime('2026-01-01 ' + temp_split[0])\n",
    "    df['Shift End'] = (dummy_date + duration_td).dt.time\n",
    "\n",
    "    # drop the source column\n",
    "    df = df.drop(columns=[\"level_3\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# apply the function to your 'grouped_wbk' dataframe\n",
    "grouped_wbk = parse_shift_times(grouped_wbk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1ffbb",
   "metadata": {},
   "source": [
    "# Create JSON Representation for Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b78fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the color palette by site\n",
    "# (use limited palette, sites by subregion)\n",
    "site_colors = {\n",
    "    # Downtown Seattle\n",
    "    \"Mary's Place\":\"#172B69\",\n",
    "    \"St. James Cathedral\":\"#1B8477\",\n",
    "    \"Youthcare Orion\":\"#A5C04D\",\n",
    "    \"Compass Day Center\":\"#FFD600\",\n",
    "    # East King County\n",
    "    \"Bellevue Library\":\"#172B69\",\n",
    "    \"Issaquah Community Hall\":\"#1B8477\",\n",
    "    \"Overlake Christian Church\":\"#A5C04D\",\n",
    "    \"Kirkland Library\":\"#FFD600\",\n",
    "    # Family Phone Line\n",
    "    \"Mary's Place Phone Line\":\"#172B69\",\n",
    "    # North King County\n",
    "    \"Shoreline Library\":\"#172B69\",\n",
    "    \"Ronald United Methodist Church\":\"#1B8477\",\n",
    "    # North Seattle\n",
    "    \"North Seattle College\":\"#172B69\",\n",
    "    \"Lake City Library\":\"#1B8477\",\n",
    "    # Snoqualmie Valley\n",
    "    \"Reclaim\":\"#172B69\",\n",
    "    \"North Bend Library\":\"#1B8477\",\n",
    "    # South East King County\n",
    "    \"Maple Valley Food Bank\":\"#172B69\",\n",
    "    \"Plateau Outreach Ministries\":\"#1B8477\",\n",
    "    # South King County\n",
    "    \"Kent Library\":\"#172B69\",\n",
    "    \"Highline United Methodist Church\":\"#1B8477\",\n",
    "    \"Federal Way Library\":\"#A5C04D\",\n",
    "    \"Renton Library\":\"#FFD600\",\n",
    "    # South Seattle\n",
    "    \"Southwest Library\":\"#172B69\",\n",
    "    \"Georgetown SVP\":\"#1B8477\",\n",
    "    \"South Lucille VA Center\":\"#A5C04D\",\n",
    "    \"South Park Library\":\"#FFD600\",\n",
    "    # Vashon Island\n",
    "    \"Vashon Island Library\":\"#172B69\",\n",
    "    \"Vashon Food Bank\":\"#1B8477\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69cc2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ISO 8601 strings for Start and End\n",
    "# assumes 'Date' is already a string or datetime object\n",
    "grouped_wbk['iso_start'] = pd.to_datetime(grouped_wbk['Date'] \\\n",
    "                                          .astype(str) + \\\n",
    "                                            ' ' + \\\n",
    "                                                grouped_wbk['Shift Start'] \\\n",
    "                                                    .astype(str)).dt \\\n",
    "                                                        .strftime('%Y-%m-%dT%H:%M:%S')\n",
    "grouped_wbk['iso_end'] = pd.to_datetime(grouped_wbk['Date'] \\\n",
    "                                        .astype(str) + \\\n",
    "                                            ' ' + \\\n",
    "                                                grouped_wbk['Shift End'] \\\n",
    "                                                    .astype(str)).dt \\\n",
    "                                                        .strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# convert to a list of dictionaries\n",
    "# we can include 'Count' in the title so users see how many volunteers are there\n",
    "calendar_data = []\n",
    "for _, row in grouped_wbk.iterrows():\n",
    "    calendar_data.append({\n",
    "        \"title\": f\"{row['Session description']}\\n({row['Count']})\", # use \\n for the break\n",
    "        \"start\": row['iso_start'],\n",
    "        \"end\": row['iso_end'],\n",
    "        \"backgroundColor\": site_colors.get(row['Session description'], \"#6c757d\"),\n",
    "        \"borderColor\": \"transparent\",\n",
    "        \"allDay\": False,\n",
    "        \"extendedProps\": {\n",
    "            \"site\": row['Session description'],\n",
    "            \"subregion\": row['Subregion'],\n",
    "            \"volCount\": row['Count'] # store the raw number just in case\n",
    "        }\n",
    "    })\n",
    "\n",
    "# export to a JSON file for your website\n",
    "with open('schedule.json', 'w') as f:\n",
    "    json.dump(calendar_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
